{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import PIL\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptor': 0, 'bardak': 1, 'kutu': 2}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Subset\n",
    " \n",
    "data_dir = \"C:\\\\Users\\\\ninja\\\\Desktop\\\\Projeler\\\\lidar_classification\\\\images\"\n",
    "\n",
    "# Dönüşümler\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "targets = [label for _, label in dataset]\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(split.split(torch.zeros(len(targets)), targets))\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(dataset.class_to_idx)  # {'adaptor': 0, 'bardak': 1, 'kutu': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ninja\\\\Desktop\\\\Projeler\\\\lidar_classification\\\\images'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] Sistem belirtilen dosyayı bulamıyor: 'data_dir'\n",
      "c:\\Users\\ninja\\Desktop\\Projeler\\lidar_classification\n"
     ]
    }
   ],
   "source": [
    "cd data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Volume Serial Number is 24C6-591C\n",
      "\n",
      " Directory of c:\\Users\\ninja\\Desktop\\Projeler\\lidar_classification\n",
      "\n",
      "14.10.2024  01:39    <DIR>          .\n",
      "14.10.2024  01:39    <DIR>          ..\n",
      "14.10.2024  01:39             4.820 bunu_at.ipynb\n",
      "14.10.2024  01:29            17.461 classification.ipynb\n",
      "13.10.2024  00:50             2.663 convex_ghuul.txt\n",
      "12.10.2024  18:07           114.383 convex_hull.jpg\n",
      "13.10.2024  08:36             1.357 convex_hull_kenarlar�.txt\n",
      "13.10.2024  18:12            36.895 convex_hull_plot_kutu.png\n",
      "13.10.2024  18:13            40.538 convex_hull_plot_kutu_filtered.png\n",
      "13.10.2024  12:54               759 csv_to_pandas.ipynb\n",
      "14.10.2024  01:15    <DIR>          images\n",
      "13.10.2024  18:20           392.806 lidar.ipynb\n",
      "13.10.2024  12:38               843 lidar_.txt\n",
      "13.10.2024  12:16            33.869 lidar_data.jpg\n",
      "12.10.2024  15:01                64 lidar_labels.csv\n",
      "13.10.2024  08:22           150.872 lidar_s�n�fland�rma.ipynb\n",
      "13.10.2024  12:53            12.779 lidar_veri_d�zenleme.ipynb\n",
      "12.10.2024  16:33    <DIR>          lidar_verileri\n",
      "12.10.2024  15:10             2.153 neriman2.csv\n",
      "13.10.2024  08:39            49.128 Risk_score_tahmin.jpg\n",
      "              16 File(s)        861.390 bytes\n",
      "               4 Dir(s)   3.270.299.648 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Convolutional katmanlar\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected katmanlar\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 128)  # 224x224'lik görüntüler için boyut\n",
    "        self.fc2 = nn.Linear(128, 3)              # 3 sınıf\n",
    "        \n",
    "        # Pooling, activation fonksiyonları ve dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # Çıktı boyutu: (16, 112, 112)\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # Çıktı boyutu: (32, 56, 56)\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # Çıktı boyutu: (64, 28, 28)\n",
    "        \n",
    "         \n",
    "        x = x.view(-1, 64 * 28 * 28)  \n",
    "        \n",
    "        x = self.dropout(self.relu(self.fc1(x)))  # Dropout fully connected katmanından önce\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "# Loss ve optimizer\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Model is running on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/200], Loss: 1.2704\n",
      "Epoch [2/200], Loss: 1.1832\n",
      "Epoch [3/200], Loss: 1.1096\n",
      "Epoch [4/200], Loss: 1.0951\n",
      "Epoch [5/200], Loss: 1.1016\n",
      "Epoch [6/200], Loss: 1.1038\n",
      "Epoch [7/200], Loss: 1.0890\n",
      "Epoch [8/200], Loss: 1.0967\n",
      "Epoch [9/200], Loss: 1.0891\n",
      "Epoch [10/200], Loss: 1.0802\n",
      "Epoch [11/200], Loss: 1.0708\n",
      "Epoch [12/200], Loss: 1.0696\n",
      "Epoch [13/200], Loss: 1.0356\n",
      "Epoch [14/200], Loss: 0.9842\n",
      "Epoch [15/200], Loss: 0.9522\n",
      "Epoch [16/200], Loss: 0.8988\n",
      "Epoch [17/200], Loss: 0.8018\n",
      "Epoch [18/200], Loss: 0.7448\n",
      "Epoch [19/200], Loss: 0.5943\n",
      "Epoch [20/200], Loss: 0.6654\n",
      "Epoch [21/200], Loss: 0.6028\n",
      "Epoch [22/200], Loss: 0.4999\n",
      "Epoch [23/200], Loss: 0.4693\n",
      "Epoch [24/200], Loss: 0.5116\n",
      "Epoch [25/200], Loss: 0.4215\n",
      "Epoch [26/200], Loss: 0.3553\n",
      "Epoch [27/200], Loss: 0.3308\n",
      "Epoch [28/200], Loss: 0.3353\n",
      "Epoch [29/200], Loss: 0.2723\n",
      "Epoch [30/200], Loss: 0.2854\n",
      "Epoch [31/200], Loss: 0.2762\n",
      "Epoch [32/200], Loss: 0.1616\n",
      "Epoch [33/200], Loss: 0.1449\n",
      "Epoch [34/200], Loss: 0.0828\n",
      "Epoch [35/200], Loss: 0.1171\n",
      "Epoch [36/200], Loss: 0.1108\n",
      "Epoch [37/200], Loss: 0.1140\n",
      "Epoch [38/200], Loss: 0.0336\n",
      "Epoch [39/200], Loss: 0.0647\n",
      "Epoch [40/200], Loss: 0.0503\n",
      "Epoch [41/200], Loss: 0.0476\n",
      "Epoch [42/200], Loss: 0.0744\n",
      "Epoch [43/200], Loss: 0.0442\n",
      "Epoch [44/200], Loss: 0.0733\n",
      "Epoch [45/200], Loss: 0.0133\n",
      "Epoch [46/200], Loss: 0.0073\n",
      "Epoch [47/200], Loss: 0.0438\n",
      "Epoch [48/200], Loss: 0.0301\n",
      "Epoch [49/200], Loss: 0.0162\n",
      "Epoch [50/200], Loss: 0.0728\n",
      "Epoch [51/200], Loss: 0.0461\n",
      "Epoch [52/200], Loss: 0.0080\n",
      "Epoch [53/200], Loss: 0.0157\n",
      "Epoch [54/200], Loss: 0.0154\n",
      "Epoch [55/200], Loss: 0.0062\n",
      "Epoch [56/200], Loss: 0.0539\n",
      "Epoch [57/200], Loss: 0.0398\n",
      "Epoch [58/200], Loss: 0.0320\n",
      "Epoch [59/200], Loss: 0.0151\n",
      "Epoch [60/200], Loss: 0.0084\n",
      "Epoch [61/200], Loss: 0.0113\n",
      "Epoch [62/200], Loss: 0.0047\n",
      "Epoch [63/200], Loss: 0.0084\n",
      "Epoch [64/200], Loss: 0.0033\n",
      "Epoch [65/200], Loss: 0.0059\n",
      "Epoch [66/200], Loss: 0.0469\n",
      "Epoch [67/200], Loss: 0.0027\n",
      "Epoch [68/200], Loss: 0.0238\n",
      "Epoch [69/200], Loss: 0.0044\n",
      "Epoch [70/200], Loss: 0.0092\n",
      "Epoch [71/200], Loss: 0.0104\n",
      "Epoch [72/200], Loss: 0.0257\n",
      "Epoch [73/200], Loss: 0.0021\n",
      "Epoch [74/200], Loss: 0.0029\n",
      "Epoch [75/200], Loss: 0.0016\n",
      "Epoch [76/200], Loss: 0.0042\n",
      "Epoch [77/200], Loss: 0.0071\n",
      "Epoch [78/200], Loss: 0.0072\n",
      "Epoch [79/200], Loss: 0.0012\n",
      "Epoch [80/200], Loss: 0.0025\n",
      "Epoch [81/200], Loss: 0.0169\n",
      "Epoch [82/200], Loss: 0.0002\n",
      "Epoch [83/200], Loss: 0.0005\n",
      "Epoch [84/200], Loss: 0.0074\n",
      "Epoch [85/200], Loss: 0.0031\n",
      "Epoch [86/200], Loss: 0.0002\n",
      "Epoch [87/200], Loss: 0.0017\n",
      "Epoch [88/200], Loss: 0.0013\n",
      "Epoch [89/200], Loss: 0.0029\n",
      "Epoch [90/200], Loss: 0.0176\n",
      "Epoch [91/200], Loss: 0.0005\n",
      "Epoch [92/200], Loss: 0.0005\n",
      "Epoch [93/200], Loss: 0.0035\n",
      "Epoch [94/200], Loss: 0.0014\n",
      "Epoch [95/200], Loss: 0.0011\n",
      "Epoch [96/200], Loss: 0.0066\n",
      "Epoch [97/200], Loss: 0.0010\n",
      "Epoch [98/200], Loss: 0.0008\n",
      "Epoch [99/200], Loss: 0.0007\n",
      "Epoch [100/200], Loss: 0.0006\n",
      "Epoch [101/200], Loss: 0.0054\n",
      "Epoch [102/200], Loss: 0.0005\n",
      "Epoch [103/200], Loss: 0.0007\n",
      "Epoch [104/200], Loss: 0.0002\n",
      "Epoch [105/200], Loss: 0.0056\n",
      "Epoch [106/200], Loss: 0.0001\n",
      "Epoch [107/200], Loss: 0.0003\n",
      "Epoch [108/200], Loss: 0.0013\n",
      "Epoch [109/200], Loss: 0.0041\n",
      "Epoch [110/200], Loss: 0.0011\n",
      "Epoch [111/200], Loss: 0.0027\n",
      "Epoch [112/200], Loss: 0.0025\n",
      "Epoch [113/200], Loss: 0.0008\n",
      "Epoch [114/200], Loss: 0.0002\n",
      "Epoch [115/200], Loss: 0.0172\n",
      "Epoch [116/200], Loss: 0.0002\n",
      "Epoch [117/200], Loss: 0.0210\n",
      "Epoch [118/200], Loss: 0.0004\n",
      "Epoch [119/200], Loss: 0.0009\n",
      "Epoch [120/200], Loss: 0.0048\n",
      "Epoch [121/200], Loss: 0.0082\n",
      "Epoch [122/200], Loss: 0.0024\n",
      "Epoch [123/200], Loss: 0.0004\n",
      "Epoch [124/200], Loss: 0.0023\n",
      "Epoch [125/200], Loss: 0.0001\n",
      "Epoch [126/200], Loss: 0.0014\n",
      "Epoch [127/200], Loss: 0.0001\n",
      "Epoch [128/200], Loss: 0.0003\n",
      "Epoch [129/200], Loss: 0.0002\n",
      "Epoch [130/200], Loss: 0.0005\n",
      "Epoch [131/200], Loss: 0.0021\n",
      "Epoch [132/200], Loss: 0.0044\n",
      "Epoch [133/200], Loss: 0.0001\n",
      "Epoch [134/200], Loss: 0.0002\n",
      "Epoch [135/200], Loss: 0.0003\n",
      "Epoch [136/200], Loss: 0.0006\n",
      "Epoch [137/200], Loss: 0.0003\n",
      "Epoch [138/200], Loss: 0.0013\n",
      "Epoch [139/200], Loss: 0.0312\n",
      "Epoch [140/200], Loss: 0.0002\n",
      "Epoch [141/200], Loss: 0.0032\n",
      "Epoch [142/200], Loss: 0.0483\n",
      "Epoch [143/200], Loss: 0.0105\n",
      "Epoch [144/200], Loss: 0.0017\n",
      "Epoch [145/200], Loss: 0.0056\n",
      "Epoch [146/200], Loss: 0.0124\n",
      "Epoch [147/200], Loss: 0.0028\n",
      "Epoch [148/200], Loss: 0.0006\n",
      "Epoch [149/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0022\n",
      "Epoch [151/200], Loss: 0.0028\n",
      "Epoch [152/200], Loss: 0.0037\n",
      "Epoch [153/200], Loss: 0.0013\n",
      "Epoch [154/200], Loss: 0.0012\n",
      "Epoch [155/200], Loss: 0.0004\n",
      "Epoch [156/200], Loss: 0.0005\n",
      "Epoch [157/200], Loss: 0.0019\n",
      "Epoch [158/200], Loss: 0.0002\n",
      "Epoch [159/200], Loss: 0.0004\n",
      "Epoch [160/200], Loss: 0.0003\n",
      "Epoch [161/200], Loss: 0.0007\n",
      "Epoch [162/200], Loss: 0.0000\n",
      "Epoch [163/200], Loss: 0.0011\n",
      "Epoch [164/200], Loss: 0.0001\n",
      "Epoch [165/200], Loss: 0.0002\n",
      "Epoch [166/200], Loss: 0.0000\n",
      "Epoch [167/200], Loss: 0.0010\n",
      "Epoch [168/200], Loss: 0.0057\n",
      "Epoch [169/200], Loss: 0.0004\n",
      "Epoch [170/200], Loss: 0.0001\n",
      "Epoch [171/200], Loss: 0.0000\n",
      "Epoch [172/200], Loss: 0.0053\n",
      "Epoch [173/200], Loss: 0.0020\n",
      "Epoch [174/200], Loss: 0.0001\n",
      "Epoch [175/200], Loss: 0.0000\n",
      "Epoch [176/200], Loss: 0.0000\n",
      "Epoch [177/200], Loss: 0.0002\n",
      "Epoch [178/200], Loss: 0.0000\n",
      "Epoch [179/200], Loss: 0.0000\n",
      "Epoch [180/200], Loss: 0.0085\n",
      "Epoch [181/200], Loss: 0.0000\n",
      "Epoch [182/200], Loss: 0.0002\n",
      "Epoch [183/200], Loss: 0.0000\n",
      "Epoch [184/200], Loss: 0.0011\n",
      "Epoch [185/200], Loss: 0.0001\n",
      "Epoch [186/200], Loss: 0.0000\n",
      "Epoch [187/200], Loss: 0.0170\n",
      "Epoch [188/200], Loss: 0.0003\n",
      "Epoch [189/200], Loss: 0.0011\n",
      "Epoch [190/200], Loss: 0.0090\n",
      "Epoch [191/200], Loss: 0.0016\n",
      "Epoch [192/200], Loss: 0.0022\n",
      "Epoch [193/200], Loss: 0.0001\n",
      "Epoch [194/200], Loss: 0.0022\n",
      "Epoch [195/200], Loss: 0.0000\n",
      "Epoch [196/200], Loss: 0.0046\n",
      "Epoch [197/200], Loss: 0.0311\n",
      "Epoch [198/200], Loss: 0.0004\n",
      "Epoch [199/200], Loss: 0.0001\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "Test Accuracy: 50.00%\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [1/200], Loss: 1.3447\n",
      "Epoch [2/200], Loss: 1.1272\n",
      "Epoch [3/200], Loss: 1.0953\n",
      "Epoch [4/200], Loss: 1.1001\n",
      "Epoch [5/200], Loss: 1.0953\n",
      "Epoch [6/200], Loss: 1.0971\n",
      "Epoch [7/200], Loss: 1.1052\n",
      "Epoch [8/200], Loss: 1.0794\n",
      "Epoch [9/200], Loss: 1.0829\n",
      "Epoch [10/200], Loss: 1.0594\n",
      "Epoch [11/200], Loss: 1.0234\n",
      "Epoch [12/200], Loss: 0.9426\n",
      "Epoch [13/200], Loss: 0.8285\n",
      "Epoch [14/200], Loss: 0.6927\n",
      "Epoch [15/200], Loss: 0.7585\n",
      "Epoch [16/200], Loss: 0.5524\n",
      "Epoch [17/200], Loss: 0.4397\n",
      "Epoch [18/200], Loss: 0.5283\n",
      "Epoch [19/200], Loss: 0.4349\n",
      "Epoch [20/200], Loss: 0.3254\n",
      "Epoch [21/200], Loss: 0.4566\n",
      "Epoch [22/200], Loss: 0.3066\n",
      "Epoch [23/200], Loss: 0.2045\n",
      "Epoch [24/200], Loss: 0.1903\n",
      "Epoch [25/200], Loss: 0.1889\n",
      "Epoch [26/200], Loss: 0.2271\n",
      "Epoch [27/200], Loss: 0.2391\n",
      "Epoch [28/200], Loss: 0.2551\n",
      "Epoch [29/200], Loss: 0.1795\n",
      "Epoch [30/200], Loss: 0.1084\n",
      "Epoch [31/200], Loss: 0.1109\n",
      "Epoch [32/200], Loss: 0.1026\n",
      "Epoch [33/200], Loss: 0.1336\n",
      "Epoch [34/200], Loss: 0.0631\n",
      "Epoch [35/200], Loss: 0.0949\n",
      "Epoch [36/200], Loss: 0.0295\n",
      "Epoch [37/200], Loss: 0.1066\n",
      "Epoch [38/200], Loss: 0.0267\n",
      "Epoch [39/200], Loss: 0.0391\n",
      "Epoch [40/200], Loss: 0.0199\n",
      "Epoch [41/200], Loss: 0.0116\n",
      "Epoch [42/200], Loss: 0.0137\n",
      "Epoch [43/200], Loss: 0.0295\n",
      "Epoch [44/200], Loss: 0.0258\n",
      "Epoch [45/200], Loss: 0.1535\n",
      "Epoch [46/200], Loss: 0.0266\n",
      "Epoch [47/200], Loss: 0.0311\n",
      "Epoch [48/200], Loss: 0.0249\n",
      "Epoch [49/200], Loss: 0.0401\n",
      "Epoch [50/200], Loss: 0.0456\n",
      "Epoch [51/200], Loss: 0.0226\n",
      "Epoch [52/200], Loss: 0.0505\n",
      "Epoch [53/200], Loss: 0.0326\n",
      "Epoch [54/200], Loss: 0.0072\n",
      "Epoch [55/200], Loss: 0.0073\n",
      "Epoch [56/200], Loss: 0.0105\n",
      "Epoch [57/200], Loss: 0.0029\n",
      "Epoch [58/200], Loss: 0.0318\n",
      "Epoch [59/200], Loss: 0.0575\n",
      "Epoch [60/200], Loss: 0.0156\n",
      "Epoch [61/200], Loss: 0.0156\n",
      "Epoch [62/200], Loss: 0.0556\n",
      "Epoch [63/200], Loss: 0.0393\n",
      "Epoch [64/200], Loss: 0.0224\n",
      "Epoch [65/200], Loss: 0.0154\n",
      "Epoch [66/200], Loss: 0.0054\n",
      "Epoch [67/200], Loss: 0.0393\n",
      "Epoch [68/200], Loss: 0.0134\n",
      "Epoch [69/200], Loss: 0.0132\n",
      "Epoch [70/200], Loss: 0.0045\n",
      "Epoch [71/200], Loss: 0.0003\n",
      "Epoch [72/200], Loss: 0.0011\n",
      "Epoch [73/200], Loss: 0.0015\n",
      "Epoch [74/200], Loss: 0.0009\n",
      "Epoch [75/200], Loss: 0.0016\n",
      "Epoch [76/200], Loss: 0.0075\n",
      "Epoch [77/200], Loss: 0.0010\n",
      "Epoch [78/200], Loss: 0.0007\n",
      "Epoch [79/200], Loss: 0.0009\n",
      "Epoch [80/200], Loss: 0.0006\n",
      "Epoch [81/200], Loss: 0.0056\n",
      "Epoch [82/200], Loss: 0.0010\n",
      "Epoch [83/200], Loss: 0.0003\n",
      "Epoch [84/200], Loss: 0.0042\n",
      "Epoch [85/200], Loss: 0.0002\n",
      "Epoch [86/200], Loss: 0.0001\n",
      "Epoch [87/200], Loss: 0.0033\n",
      "Epoch [88/200], Loss: 0.0030\n",
      "Epoch [89/200], Loss: 0.0024\n",
      "Epoch [90/200], Loss: 0.0004\n",
      "Epoch [91/200], Loss: 0.0035\n",
      "Epoch [92/200], Loss: 0.0089\n",
      "Epoch [93/200], Loss: 0.0020\n",
      "Epoch [94/200], Loss: 0.0019\n",
      "Epoch [95/200], Loss: 0.0001\n",
      "Epoch [96/200], Loss: 0.0116\n",
      "Epoch [97/200], Loss: 0.0003\n",
      "Epoch [98/200], Loss: 0.0009\n",
      "Epoch [99/200], Loss: 0.0009\n",
      "Epoch [100/200], Loss: 0.0001\n",
      "Epoch [101/200], Loss: 0.0002\n",
      "Epoch [102/200], Loss: 0.0006\n",
      "Epoch [103/200], Loss: 0.0000\n",
      "Epoch [104/200], Loss: 0.0001\n",
      "Epoch [105/200], Loss: 0.0002\n",
      "Epoch [106/200], Loss: 0.0002\n",
      "Epoch [107/200], Loss: 0.0023\n",
      "Epoch [108/200], Loss: 0.0001\n",
      "Epoch [109/200], Loss: 0.0002\n",
      "Epoch [110/200], Loss: 0.0001\n",
      "Epoch [111/200], Loss: 0.0097\n",
      "Epoch [112/200], Loss: 0.0001\n",
      "Epoch [113/200], Loss: 0.0003\n",
      "Epoch [114/200], Loss: 0.0001\n",
      "Epoch [115/200], Loss: 0.0001\n",
      "Epoch [116/200], Loss: 0.0002\n",
      "Epoch [117/200], Loss: 0.0184\n",
      "Epoch [118/200], Loss: 0.0033\n",
      "Epoch [119/200], Loss: 0.0165\n",
      "Epoch [120/200], Loss: 0.0040\n",
      "Epoch [121/200], Loss: 0.0009\n",
      "Epoch [122/200], Loss: 0.0028\n",
      "Epoch [123/200], Loss: 0.0050\n",
      "Epoch [124/200], Loss: 0.0018\n",
      "Epoch [125/200], Loss: 0.0015\n",
      "Epoch [126/200], Loss: 0.0012\n",
      "Epoch [127/200], Loss: 0.0001\n",
      "Epoch [128/200], Loss: 0.0378\n",
      "Epoch [129/200], Loss: 0.0003\n",
      "Epoch [130/200], Loss: 0.0008\n",
      "Epoch [131/200], Loss: 0.0022\n",
      "Epoch [132/200], Loss: 0.0036\n",
      "Epoch [133/200], Loss: 0.0021\n",
      "Epoch [134/200], Loss: 0.0003\n",
      "Epoch [135/200], Loss: 0.0004\n",
      "Epoch [136/200], Loss: 0.0010\n",
      "Epoch [137/200], Loss: 0.0001\n",
      "Epoch [138/200], Loss: 0.0002\n",
      "Epoch [139/200], Loss: 0.0000\n",
      "Epoch [140/200], Loss: 0.0002\n",
      "Epoch [141/200], Loss: 0.0003\n",
      "Epoch [142/200], Loss: 0.0023\n",
      "Epoch [143/200], Loss: 0.0026\n",
      "Epoch [144/200], Loss: 0.0027\n",
      "Epoch [145/200], Loss: 0.0003\n",
      "Epoch [146/200], Loss: 0.0001\n",
      "Epoch [147/200], Loss: 0.0044\n",
      "Epoch [148/200], Loss: 0.0001\n",
      "Epoch [149/200], Loss: 0.0003\n",
      "Epoch [150/200], Loss: 0.0002\n",
      "Epoch [151/200], Loss: 0.0000\n",
      "Epoch [152/200], Loss: 0.0001\n",
      "Epoch [153/200], Loss: 0.0001\n",
      "Epoch [154/200], Loss: 0.0000\n",
      "Epoch [155/200], Loss: 0.0020\n",
      "Epoch [156/200], Loss: 0.0003\n",
      "Epoch [157/200], Loss: 0.0010\n",
      "Epoch [158/200], Loss: 0.0001\n",
      "Epoch [159/200], Loss: 0.0108\n",
      "Epoch [160/200], Loss: 0.0000\n",
      "Epoch [161/200], Loss: 0.0002\n",
      "Epoch [162/200], Loss: 0.0029\n",
      "Epoch [163/200], Loss: 0.0003\n",
      "Epoch [164/200], Loss: 0.0007\n",
      "Epoch [165/200], Loss: 0.0003\n",
      "Epoch [166/200], Loss: 0.0000\n",
      "Epoch [167/200], Loss: 0.0002\n",
      "Epoch [168/200], Loss: 0.0007\n",
      "Epoch [169/200], Loss: 0.0024\n",
      "Epoch [170/200], Loss: 0.0001\n",
      "Epoch [171/200], Loss: 0.0001\n",
      "Epoch [172/200], Loss: 0.0021\n",
      "Epoch [173/200], Loss: 0.0002\n",
      "Epoch [174/200], Loss: 0.0020\n",
      "Epoch [175/200], Loss: 0.0021\n",
      "Epoch [176/200], Loss: 0.0000\n",
      "Epoch [177/200], Loss: 0.0000\n",
      "Epoch [178/200], Loss: 0.0001\n",
      "Epoch [179/200], Loss: 0.0115\n",
      "Epoch [180/200], Loss: 0.0095\n",
      "Epoch [181/200], Loss: 0.0019\n",
      "Epoch [182/200], Loss: 0.0000\n",
      "Epoch [183/200], Loss: 0.0001\n",
      "Epoch [184/200], Loss: 0.0004\n",
      "Epoch [185/200], Loss: 0.0001\n",
      "Epoch [186/200], Loss: 0.0012\n",
      "Epoch [187/200], Loss: 0.0000\n",
      "Epoch [188/200], Loss: 0.0002\n",
      "Epoch [189/200], Loss: 0.0047\n",
      "Epoch [190/200], Loss: 0.0003\n",
      "Epoch [191/200], Loss: 0.0001\n",
      "Epoch [192/200], Loss: 0.0002\n",
      "Epoch [193/200], Loss: 0.0000\n",
      "Epoch [194/200], Loss: 0.0003\n",
      "Epoch [195/200], Loss: 0.0000\n",
      "Epoch [196/200], Loss: 0.0042\n",
      "Epoch [197/200], Loss: 0.0006\n",
      "Epoch [198/200], Loss: 0.0000\n",
      "Epoch [199/200], Loss: 0.0329\n",
      "Epoch [200/200], Loss: 0.0000\n",
      "Test Accuracy: 50.00%\n",
      "\n",
      "Fold 3/5\n",
      "Epoch [1/200], Loss: 1.6338\n",
      "Epoch [2/200], Loss: 1.2596\n",
      "Epoch [3/200], Loss: 1.1213\n",
      "Epoch [4/200], Loss: 1.0886\n",
      "Epoch [5/200], Loss: 1.0919\n",
      "Epoch [6/200], Loss: 1.0979\n",
      "Epoch [7/200], Loss: 1.1028\n",
      "Epoch [8/200], Loss: 1.0942\n",
      "Epoch [9/200], Loss: 1.0932\n",
      "Epoch [10/200], Loss: 1.0888\n",
      "Epoch [11/200], Loss: 1.0906\n",
      "Epoch [12/200], Loss: 1.0797\n",
      "Epoch [13/200], Loss: 1.0646\n",
      "Epoch [14/200], Loss: 1.0609\n",
      "Epoch [15/200], Loss: 1.0575\n",
      "Epoch [16/200], Loss: 0.9952\n",
      "Epoch [17/200], Loss: 1.0105\n",
      "Epoch [18/200], Loss: 0.9473\n",
      "Epoch [19/200], Loss: 0.8818\n",
      "Epoch [20/200], Loss: 0.7585\n",
      "Epoch [21/200], Loss: 0.8872\n",
      "Epoch [22/200], Loss: 0.7286\n",
      "Epoch [23/200], Loss: 0.6354\n",
      "Epoch [24/200], Loss: 0.5487\n",
      "Epoch [25/200], Loss: 0.5699\n",
      "Epoch [26/200], Loss: 0.4808\n",
      "Epoch [27/200], Loss: 0.5465\n",
      "Epoch [28/200], Loss: 0.4681\n",
      "Epoch [29/200], Loss: 0.4119\n",
      "Epoch [30/200], Loss: 0.2879\n",
      "Epoch [31/200], Loss: 0.2764\n",
      "Epoch [32/200], Loss: 0.2018\n",
      "Epoch [33/200], Loss: 0.1721\n",
      "Epoch [34/200], Loss: 0.2004\n",
      "Epoch [35/200], Loss: 0.1046\n",
      "Epoch [36/200], Loss: 0.1175\n",
      "Epoch [37/200], Loss: 0.1176\n",
      "Epoch [38/200], Loss: 0.0990\n",
      "Epoch [39/200], Loss: 0.0588\n",
      "Epoch [40/200], Loss: 0.0828\n",
      "Epoch [41/200], Loss: 0.0365\n",
      "Epoch [42/200], Loss: 0.0437\n",
      "Epoch [43/200], Loss: 0.0154\n",
      "Epoch [44/200], Loss: 0.0201\n",
      "Epoch [45/200], Loss: 0.0247\n",
      "Epoch [46/200], Loss: 0.0351\n",
      "Epoch [47/200], Loss: 0.0095\n",
      "Epoch [48/200], Loss: 0.0651\n",
      "Epoch [49/200], Loss: 0.0077\n",
      "Epoch [50/200], Loss: 0.0100\n",
      "Epoch [51/200], Loss: 0.0129\n",
      "Epoch [52/200], Loss: 0.0524\n",
      "Epoch [53/200], Loss: 0.0109\n",
      "Epoch [54/200], Loss: 0.0022\n",
      "Epoch [55/200], Loss: 0.0082\n",
      "Epoch [56/200], Loss: 0.0081\n",
      "Epoch [57/200], Loss: 0.0370\n",
      "Epoch [58/200], Loss: 0.0060\n",
      "Epoch [59/200], Loss: 0.0081\n",
      "Epoch [60/200], Loss: 0.0069\n",
      "Epoch [61/200], Loss: 0.0019\n",
      "Epoch [62/200], Loss: 0.0048\n",
      "Epoch [63/200], Loss: 0.0029\n",
      "Epoch [64/200], Loss: 0.0008\n",
      "Epoch [65/200], Loss: 0.0080\n",
      "Epoch [66/200], Loss: 0.0008\n",
      "Epoch [67/200], Loss: 0.0027\n",
      "Epoch [68/200], Loss: 0.0103\n",
      "Epoch [69/200], Loss: 0.0018\n",
      "Epoch [70/200], Loss: 0.0013\n",
      "Epoch [71/200], Loss: 0.0041\n",
      "Epoch [72/200], Loss: 0.0251\n",
      "Epoch [73/200], Loss: 0.0180\n",
      "Epoch [74/200], Loss: 0.0007\n",
      "Epoch [75/200], Loss: 0.0002\n",
      "Epoch [76/200], Loss: 0.0004\n",
      "Epoch [77/200], Loss: 0.0018\n",
      "Epoch [78/200], Loss: 0.0007\n",
      "Epoch [79/200], Loss: 0.0252\n",
      "Epoch [80/200], Loss: 0.0014\n",
      "Epoch [81/200], Loss: 0.0010\n",
      "Epoch [82/200], Loss: 0.0004\n",
      "Epoch [83/200], Loss: 0.0093\n",
      "Epoch [84/200], Loss: 0.0012\n",
      "Epoch [85/200], Loss: 0.0060\n",
      "Epoch [86/200], Loss: 0.0013\n",
      "Epoch [87/200], Loss: 0.0089\n",
      "Epoch [88/200], Loss: 0.0262\n",
      "Epoch [89/200], Loss: 0.0017\n",
      "Epoch [90/200], Loss: 0.0014\n",
      "Epoch [91/200], Loss: 0.0016\n",
      "Epoch [92/200], Loss: 0.0013\n",
      "Epoch [93/200], Loss: 0.0004\n",
      "Epoch [94/200], Loss: 0.0008\n",
      "Epoch [95/200], Loss: 0.0130\n",
      "Epoch [96/200], Loss: 0.0006\n",
      "Epoch [97/200], Loss: 0.0008\n",
      "Epoch [98/200], Loss: 0.0002\n",
      "Epoch [99/200], Loss: 0.0022\n",
      "Epoch [100/200], Loss: 0.0210\n",
      "Epoch [101/200], Loss: 0.0011\n",
      "Epoch [102/200], Loss: 0.0005\n",
      "Epoch [103/200], Loss: 0.0011\n",
      "Epoch [104/200], Loss: 0.0076\n",
      "Epoch [105/200], Loss: 0.0026\n",
      "Epoch [106/200], Loss: 0.0005\n",
      "Epoch [107/200], Loss: 0.0001\n",
      "Epoch [108/200], Loss: 0.0004\n",
      "Epoch [109/200], Loss: 0.0002\n",
      "Epoch [110/200], Loss: 0.0138\n",
      "Epoch [111/200], Loss: 0.0025\n",
      "Epoch [112/200], Loss: 0.0138\n",
      "Epoch [113/200], Loss: 0.0006\n",
      "Epoch [114/200], Loss: 0.0008\n",
      "Epoch [115/200], Loss: 0.0010\n",
      "Epoch [116/200], Loss: 0.0036\n",
      "Epoch [117/200], Loss: 0.0035\n",
      "Epoch [118/200], Loss: 0.0004\n",
      "Epoch [119/200], Loss: 0.0050\n",
      "Epoch [120/200], Loss: 0.0349\n",
      "Epoch [121/200], Loss: 0.0467\n",
      "Epoch [122/200], Loss: 0.0013\n",
      "Epoch [123/200], Loss: 0.0074\n",
      "Epoch [124/200], Loss: 0.0048\n",
      "Epoch [125/200], Loss: 0.0028\n",
      "Epoch [126/200], Loss: 0.0062\n",
      "Epoch [127/200], Loss: 0.0019\n",
      "Epoch [128/200], Loss: 0.0018\n",
      "Epoch [129/200], Loss: 0.0010\n",
      "Epoch [130/200], Loss: 0.0025\n",
      "Epoch [131/200], Loss: 0.0021\n",
      "Epoch [132/200], Loss: 0.0008\n",
      "Epoch [133/200], Loss: 0.0002\n",
      "Epoch [134/200], Loss: 0.0019\n",
      "Epoch [135/200], Loss: 0.0012\n",
      "Epoch [136/200], Loss: 0.0009\n",
      "Epoch [137/200], Loss: 0.0028\n",
      "Epoch [138/200], Loss: 0.0000\n",
      "Epoch [139/200], Loss: 0.0018\n",
      "Epoch [140/200], Loss: 0.0001\n",
      "Epoch [141/200], Loss: 0.0001\n",
      "Epoch [142/200], Loss: 0.0005\n",
      "Epoch [143/200], Loss: 0.0003\n",
      "Epoch [144/200], Loss: 0.0019\n",
      "Epoch [145/200], Loss: 0.0004\n",
      "Epoch [146/200], Loss: 0.0004\n",
      "Epoch [147/200], Loss: 0.0001\n",
      "Epoch [148/200], Loss: 0.0013\n",
      "Epoch [149/200], Loss: 0.0002\n",
      "Epoch [150/200], Loss: 0.0003\n",
      "Epoch [151/200], Loss: 0.0005\n",
      "Epoch [152/200], Loss: 0.0282\n",
      "Epoch [153/200], Loss: 0.0060\n",
      "Epoch [154/200], Loss: 0.0005\n",
      "Epoch [155/200], Loss: 0.0000\n",
      "Epoch [156/200], Loss: 0.0196\n",
      "Epoch [157/200], Loss: 0.0019\n",
      "Epoch [158/200], Loss: 0.0018\n",
      "Epoch [159/200], Loss: 0.0006\n",
      "Epoch [160/200], Loss: 0.0026\n",
      "Epoch [161/200], Loss: 0.0236\n",
      "Epoch [162/200], Loss: 0.0025\n",
      "Epoch [163/200], Loss: 0.0047\n",
      "Epoch [164/200], Loss: 0.0033\n",
      "Epoch [165/200], Loss: 0.0010\n",
      "Epoch [166/200], Loss: 0.0001\n",
      "Epoch [167/200], Loss: 0.0003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 27\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Test döngüsü\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = CNNModel().to(device)  # \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    num_epochs = 200  \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "    \n",
    "   \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%\\n')\n",
    "\n",
    "print(\"Cross-validation tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8630, Accuracy: 35.42%\n",
      "Epoch [2/100], Loss: 0.8297, Accuracy: 31.25%\n",
      "Epoch [3/100], Loss: 0.8283, Accuracy: 29.17%\n",
      "Epoch [4/100], Loss: 0.8226, Accuracy: 29.17%\n",
      "Epoch [5/100], Loss: 0.8239, Accuracy: 29.17%\n",
      "Epoch [6/100], Loss: 0.8225, Accuracy: 35.42%\n",
      "Epoch [7/100], Loss: 0.8222, Accuracy: 35.42%\n",
      "Epoch [8/100], Loss: 0.8219, Accuracy: 35.42%\n",
      "Epoch [9/100], Loss: 0.8216, Accuracy: 35.42%\n",
      "Epoch [10/100], Loss: 0.8208, Accuracy: 35.42%\n",
      "Epoch [11/100], Loss: 0.8204, Accuracy: 35.42%\n",
      "Epoch [12/100], Loss: 0.8206, Accuracy: 35.42%\n",
      "Epoch [13/100], Loss: 0.8193, Accuracy: 35.42%\n",
      "Epoch [14/100], Loss: 0.8178, Accuracy: 50.00%\n",
      "Epoch [15/100], Loss: 0.8200, Accuracy: 39.58%\n",
      "Epoch [16/100], Loss: 0.8139, Accuracy: 41.67%\n",
      "Epoch [17/100], Loss: 0.8093, Accuracy: 43.75%\n",
      "Epoch [18/100], Loss: 0.8178, Accuracy: 43.75%\n",
      "Epoch [19/100], Loss: 0.8105, Accuracy: 39.58%\n",
      "Epoch [20/100], Loss: 0.8094, Accuracy: 39.58%\n",
      "Epoch [21/100], Loss: 0.8059, Accuracy: 45.83%\n",
      "Epoch [22/100], Loss: 0.8133, Accuracy: 39.58%\n",
      "Epoch [23/100], Loss: 0.8025, Accuracy: 50.00%\n",
      "Epoch [24/100], Loss: 0.7860, Accuracy: 50.00%\n",
      "Epoch [25/100], Loss: 0.7770, Accuracy: 47.92%\n",
      "Epoch [26/100], Loss: 0.7714, Accuracy: 54.17%\n",
      "Epoch [27/100], Loss: 0.7383, Accuracy: 56.25%\n",
      "Epoch [28/100], Loss: 0.7209, Accuracy: 50.00%\n",
      "Epoch [29/100], Loss: 0.6785, Accuracy: 56.25%\n",
      "Epoch [30/100], Loss: 0.6880, Accuracy: 58.33%\n",
      "Epoch [31/100], Loss: 0.6267, Accuracy: 70.83%\n",
      "Epoch [32/100], Loss: 0.6692, Accuracy: 66.67%\n",
      "Epoch [33/100], Loss: 0.5950, Accuracy: 70.83%\n",
      "Epoch [34/100], Loss: 0.5704, Accuracy: 72.92%\n",
      "Epoch [35/100], Loss: 0.5930, Accuracy: 75.00%\n",
      "Epoch [36/100], Loss: 0.5378, Accuracy: 77.08%\n",
      "Epoch [37/100], Loss: 0.5142, Accuracy: 75.00%\n",
      "Epoch [38/100], Loss: 0.4652, Accuracy: 81.25%\n",
      "Epoch [39/100], Loss: 0.4359, Accuracy: 75.00%\n",
      "Epoch [40/100], Loss: 0.3740, Accuracy: 75.00%\n",
      "Epoch [41/100], Loss: 0.3980, Accuracy: 77.08%\n",
      "Epoch [42/100], Loss: 0.4010, Accuracy: 85.42%\n",
      "Epoch [43/100], Loss: 0.3699, Accuracy: 87.50%\n",
      "Epoch [44/100], Loss: 0.3512, Accuracy: 85.42%\n",
      "Epoch [45/100], Loss: 0.3080, Accuracy: 87.50%\n",
      "Epoch [46/100], Loss: 0.3531, Accuracy: 87.50%\n",
      "Epoch [47/100], Loss: 0.2992, Accuracy: 89.58%\n",
      "Epoch [48/100], Loss: 0.2655, Accuracy: 87.50%\n",
      "Epoch [49/100], Loss: 0.2500, Accuracy: 87.50%\n",
      "Epoch [50/100], Loss: 0.2169, Accuracy: 91.67%\n",
      "Epoch [51/100], Loss: 0.1954, Accuracy: 91.67%\n",
      "Epoch [52/100], Loss: 0.2392, Accuracy: 87.50%\n",
      "Epoch [53/100], Loss: 0.1852, Accuracy: 87.50%\n",
      "Epoch [54/100], Loss: 0.1694, Accuracy: 89.58%\n",
      "Epoch [55/100], Loss: 0.2132, Accuracy: 89.58%\n",
      "Epoch [56/100], Loss: 0.1453, Accuracy: 95.83%\n",
      "Epoch [57/100], Loss: 0.2510, Accuracy: 87.50%\n",
      "Epoch [58/100], Loss: 0.1484, Accuracy: 93.75%\n",
      "Epoch [59/100], Loss: 0.1475, Accuracy: 95.83%\n",
      "Epoch [60/100], Loss: 0.1962, Accuracy: 87.50%\n",
      "Epoch [61/100], Loss: 0.1050, Accuracy: 93.75%\n",
      "Epoch [62/100], Loss: 0.1550, Accuracy: 93.75%\n",
      "Epoch [63/100], Loss: 0.0966, Accuracy: 95.83%\n",
      "Epoch [64/100], Loss: 0.0560, Accuracy: 97.92%\n",
      "Epoch [65/100], Loss: 0.0950, Accuracy: 97.92%\n",
      "Epoch [66/100], Loss: 0.0806, Accuracy: 95.83%\n",
      "Epoch [67/100], Loss: 0.0647, Accuracy: 97.92%\n",
      "Epoch [68/100], Loss: 0.0883, Accuracy: 93.75%\n",
      "Epoch [69/100], Loss: 0.0572, Accuracy: 97.92%\n",
      "Epoch [70/100], Loss: 0.0567, Accuracy: 97.92%\n",
      "Epoch [71/100], Loss: 0.0365, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.0502, Accuracy: 97.92%\n",
      "Epoch [73/100], Loss: 0.0396, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.1000, Accuracy: 93.75%\n",
      "Epoch [75/100], Loss: 0.0493, Accuracy: 97.92%\n",
      "Epoch [76/100], Loss: 0.0512, Accuracy: 97.92%\n",
      "Epoch [77/100], Loss: 0.0444, Accuracy: 97.92%\n",
      "Epoch [78/100], Loss: 0.0192, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0302, Accuracy: 97.92%\n",
      "Epoch [80/100], Loss: 0.0180, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0231, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0124, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0181, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0108, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0202, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0526, Accuracy: 97.92%\n",
      "Epoch [87/100], Loss: 0.0059, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0121, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0370, Accuracy: 95.83%\n",
      "Epoch [90/100], Loss: 0.0946, Accuracy: 95.83%\n",
      "Epoch [91/100], Loss: 0.0354, Accuracy: 97.92%\n",
      "Epoch [92/100], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0059, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0243, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0222, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0152, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0146, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0139, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0028, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0054, Accuracy: 100.00%\n",
      "Eğitim tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "   \n",
    "    loss_list.append(epoch_loss)\n",
    "    accuracy_list.append(epoch_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print(\"Eğitim tamamlandı!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğruluk: 66.67%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images,labels=images.to(device),labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Doğruluk: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_lidar_classification.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CNNModel()\n",
    "model.load_state_dict(torch.load('model_lidar_classification.pth'))\n",
    "model.eval()  \n",
    "from PIL import Image\n",
    "image_path = 'C:/Users/ninja/Desktop/Projeler/lidar_classification/adaptor/some_image.png'\n",
    "img = Image.open(image_path)\n",
    "img = transform(img).unsqueeze(0)  # Görüntüyü tensor haline getir ve batch boyutu ekle\n",
    "\n",
    "output = model(img)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f'Tahmin edilen sınıf: {predicted.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptor': 0, 'bardak': 1, 'kutu': 2}\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"C:\\\\Users\\\\ninja\\\\Desktop\\\\Projeler\\\\lidar_classification\\\\images\"\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "test_size = len(dataset) - train_size  \n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(dataset.class_to_idx)  # Örnek: {'adaptor': 0, 'bardak': 1, 'kutu': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  \n",
    "num_classes = 3 \n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Son katmanı  Sınıf sayısına göre değiştiriyoruz\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0005\n",
      "Epoch [2/100], Loss: 0.0008\n",
      "Epoch [3/100], Loss: 0.0004\n",
      "Epoch [4/100], Loss: 0.0008\n",
      "Epoch [5/100], Loss: 0.0020\n",
      "Epoch [6/100], Loss: 0.0004\n",
      "Epoch [7/100], Loss: 0.0004\n",
      "Epoch [8/100], Loss: 0.0004\n",
      "Epoch [9/100], Loss: 0.0005\n",
      "Epoch [10/100], Loss: 0.0007\n",
      "Epoch [11/100], Loss: 0.0003\n",
      "Epoch [12/100], Loss: 0.0003\n",
      "Epoch [13/100], Loss: 0.0020\n",
      "Epoch [14/100], Loss: 0.0004\n",
      "Epoch [15/100], Loss: 0.0002\n",
      "Epoch [16/100], Loss: 0.0005\n",
      "Epoch [17/100], Loss: 0.0002\n",
      "Epoch [18/100], Loss: 0.0005\n",
      "Epoch [19/100], Loss: 0.0002\n",
      "Epoch [20/100], Loss: 0.0003\n",
      "Epoch [21/100], Loss: 0.0003\n",
      "Epoch [22/100], Loss: 0.0001\n",
      "Epoch [23/100], Loss: 0.0004\n",
      "Epoch [24/100], Loss: 0.0006\n",
      "Epoch [25/100], Loss: 0.0002\n",
      "Epoch [26/100], Loss: 0.0003\n",
      "Epoch [27/100], Loss: 0.0002\n",
      "Epoch [28/100], Loss: 0.0002\n",
      "Epoch [29/100], Loss: 0.0003\n",
      "Epoch [30/100], Loss: 0.0003\n",
      "Epoch [31/100], Loss: 0.0002\n",
      "Epoch [32/100], Loss: 0.0005\n",
      "Epoch [33/100], Loss: 0.0003\n",
      "Epoch [34/100], Loss: 0.0029\n",
      "Epoch [35/100], Loss: 0.0003\n",
      "Epoch [36/100], Loss: 0.0002\n",
      "Epoch [37/100], Loss: 0.0002\n",
      "Epoch [38/100], Loss: 0.0003\n",
      "Epoch [39/100], Loss: 0.0013\n",
      "Epoch [40/100], Loss: 0.0004\n",
      "Epoch [41/100], Loss: 0.0005\n",
      "Epoch [42/100], Loss: 0.0013\n",
      "Epoch [43/100], Loss: 0.0002\n",
      "Epoch [44/100], Loss: 0.0004\n",
      "Epoch [45/100], Loss: 0.0015\n",
      "Epoch [46/100], Loss: 0.0002\n",
      "Epoch [47/100], Loss: 0.0020\n",
      "Epoch [48/100], Loss: 0.0002\n",
      "Epoch [49/100], Loss: 0.0005\n",
      "Epoch [50/100], Loss: 0.0002\n",
      "Epoch [51/100], Loss: 0.0011\n",
      "Epoch [52/100], Loss: 0.0001\n",
      "Epoch [53/100], Loss: 0.0002\n",
      "Epoch [54/100], Loss: 0.0003\n",
      "Epoch [55/100], Loss: 0.0006\n",
      "Epoch [56/100], Loss: 0.0001\n",
      "Epoch [57/100], Loss: 0.0002\n",
      "Epoch [58/100], Loss: 0.0003\n",
      "Epoch [59/100], Loss: 0.0002\n",
      "Epoch [60/100], Loss: 0.0005\n",
      "Epoch [61/100], Loss: 0.0004\n",
      "Epoch [62/100], Loss: 0.0006\n",
      "Epoch [63/100], Loss: 0.0002\n",
      "Epoch [64/100], Loss: 0.0001\n",
      "Epoch [65/100], Loss: 0.0002\n",
      "Epoch [66/100], Loss: 0.0002\n",
      "Epoch [67/100], Loss: 0.0005\n",
      "Epoch [68/100], Loss: 0.0003\n",
      "Epoch [69/100], Loss: 0.0001\n",
      "Epoch [70/100], Loss: 0.0001\n",
      "Epoch [71/100], Loss: 0.0001\n",
      "Epoch [72/100], Loss: 0.0001\n",
      "Epoch [73/100], Loss: 0.0001\n",
      "Epoch [74/100], Loss: 0.0004\n",
      "Epoch [75/100], Loss: 0.0001\n",
      "Epoch [76/100], Loss: 0.0002\n",
      "Epoch [77/100], Loss: 0.0011\n",
      "Epoch [78/100], Loss: 0.0001\n",
      "Epoch [79/100], Loss: 0.0006\n",
      "Epoch [80/100], Loss: 0.0003\n",
      "Epoch [81/100], Loss: 0.0002\n",
      "Epoch [82/100], Loss: 0.0002\n",
      "Epoch [83/100], Loss: 0.0001\n",
      "Epoch [84/100], Loss: 0.0004\n",
      "Epoch [85/100], Loss: 0.0001\n",
      "Epoch [86/100], Loss: 0.0003\n",
      "Epoch [87/100], Loss: 0.0004\n",
      "Epoch [88/100], Loss: 0.0004\n",
      "Epoch [89/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [91/100], Loss: 0.0002\n",
      "Epoch [92/100], Loss: 0.0002\n",
      "Epoch [93/100], Loss: 0.0001\n",
      "Epoch [94/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0001\n",
      "Epoch [96/100], Loss: 0.0002\n",
      "Epoch [97/100], Loss: 0.0001\n",
      "Epoch [98/100], Loss: 0.0009\n",
      "Epoch [99/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0002\n",
      "Eğitim tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train() \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print(\"Eğitim tamamlandı!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.33%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
